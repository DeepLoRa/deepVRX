{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image\n",
    "import visdom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis = visdom.Visdom(env='deepVRX')\n",
    "vis.line([1.,2.],[1.,2.],win = 'pix_loss',name = 'pix_loss',opts = dict(title = 'pix_loss',legend = ['pix_loss']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showImageVis(img):\n",
    "    # 将张量转换为NumPy数组并进行反归一化\n",
    "    img = img.cpu().detach().numpy().transpose((1, 2, 0))  # 将通道维度放在最后\n",
    "    mean = np.array([0.5, 0.5, 0.5])\n",
    "    std = np.array([0.5, 0.5, 0.5])\n",
    "    img = std * img + mean\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    # 展示图像\n",
    "    vis.image(img,win = 'img')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nf = 16\n",
    "nh = 16\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Generator,self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(3,nf,kernel_size=8,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(nf),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(nf,nf*2,kernel_size=5,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(nf*2),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*2,nh,kernel_size=5,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(nh),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.convTrans1 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nh,nf*2,kernel_size=5,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(nf*2),\n",
    "            nn.ReLU()\n",
    "            )\n",
    "        self.convTrans2 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nf*4,nf,kernel_size=5,stride=2,padding=0),\n",
    "            nn.BatchNorm2d(nf),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "        self.convTrans3 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nf*2,3,kernel_size=5,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "        self.convTrans4 = nn.Sequential(\n",
    "            nn.ConvTranspose2d(3,3,kernel_size=4,stride=1,padding=0),\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.Tanh()\n",
    "            )\n",
    "    def forward(self,x):\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x1)\n",
    "        x3 = self.conv3(x2)\n",
    "        x4 = self.convTrans1(x3)\n",
    "        x5 = self.convTrans2(torch.cat((x2,x4),dim=1))\n",
    "        x6 = self.convTrans3(torch.cat((x1,x5),dim=1))\n",
    "        x7 = self.convTrans4(x6)\n",
    "        return x7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discr(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discr, self).__init__()\n",
    "\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3,stride=2),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.MaxPool2d(kernel_size=3,stride=3),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=0),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            nn.Flatten(1),\n",
    "            nn.Linear(128*3*4,1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.main(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Generator().cuda()\n",
    "G.load_state_dict(torch.load('./models/G.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Discr().cuda()\n",
    "D.load_state_dict(torch.load('./models/D.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class p2pDataset(Dataset):\n",
    "    def __init__(self, data_dir, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.noise_folder = os.path.join(data_dir, './pics2/clear1/')\n",
    "        self.clear_folder = os.path.join(data_dir, './pics2/noise1/')\n",
    "        self.image_list = os.listdir(self.noise_folder)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "\n",
    "        noise_path = os.path.join(self.noise_folder, img_name)\n",
    "        clear_path = os.path.join(self.clear_folder, img_name)\n",
    "\n",
    "        noise_img = Image.open(noise_path).convert('RGB')\n",
    "        clear_img = Image.open(clear_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            noise_img = self.transform(noise_img)\n",
    "            clear_img = self.transform(clear_img)\n",
    "\n",
    "        return noise_img, clear_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = p2pDataset('',transform=data_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20\n",
    "\n",
    "epochs = 500\n",
    "printStep = 10\n",
    "showStep = 100\n",
    "\n",
    "optG = torch.optim.Adam(G.parameters(), lr=0.008)\n",
    "optD = torch.optim.RMSprop(D.parameters(), lr=0.0005)\n",
    "\n",
    "criterionG = nn.MSELoss()\n",
    "criterionD = nn.MSELoss()\n",
    "\n",
    "losslist = []\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_labels = torch.ones(batch_size, 1).cuda()\n",
    "fake_labels = torch.zeros(batch_size, 1).cuda()\n",
    "losslist = []\n",
    "for epoch in range(epochs):\n",
    "    for step,(img,img2) in enumerate(data_loader):\n",
    "        clear_img = img.cuda()\n",
    "        noise_img = img2.cuda()\n",
    "        \n",
    "        #train D\n",
    "        real_outputs = D(clear_img)\n",
    "        real_loss = criterionD(real_outputs, real_labels)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            fake_img = G(noise_img)\n",
    "        fake_outputs = D(fake_img.detach())\n",
    "        fake_loss = criterionD(fake_outputs, fake_labels)\n",
    "        \n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward()\n",
    "        optD.step()\n",
    "        optD.zero_grad()\n",
    "        \n",
    "        #train G\n",
    "        fake_img = G(noise_img)\n",
    "        fake_outputs = D(fake_img)\n",
    "        \n",
    "        g_gan_loss = criterionD(fake_outputs, real_labels*0.9)*0.05\n",
    "        g_pix_loss = criterionG(fake_img,clear_img)*10\n",
    "        losslist.append(g_pix_loss.item())\n",
    "        g_loss = g_gan_loss+g_pix_loss\n",
    "        g_loss.backward()\n",
    "        optG.step()\n",
    "        optG.zero_grad()\n",
    "        if step % 5 == 0:\n",
    "            vis.line(Y=losslist,name='pix_loss',win='pix_loss')\n",
    "    if epoch % 1 == 0:\n",
    "        print('Epoch: {}, Step: {}, D_loss: {:.5f}, G_loss: gan {:.5f} + pix {:.5f}'.format(epoch, step, d_loss.item(), g_gan_loss.item(), g_pix_loss.item()))\n",
    "    if epoch % 1 == 0:\n",
    "        showImageVis(torch.cat((noise_img[0],fake_img[0],clear_img[0]),dim=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis.image(fake_img[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(G.parameters,'./models/G2.pth')\n",
    "torch.save(D.parameters,'./models/D2.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
